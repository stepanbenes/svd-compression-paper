\section{Introduction}
\label{sec:introduction}

Amount of data produced by a complex finite element analysis can be enormous and typical personal computer is not capable to store, process and visualize the results in reasonable time. Singular Value Decomposition is a well known factorization method that provides rich information about matrix systems. One of its many applications is the image compression where it can significantly reduce the size of the data representing an image while preserving the quality of image appearance.

The effort to reduce size of resulting data from complex finite element analyses to accelerate (or even make possible) post-processing using common personal computer is not new. In \cite{Benes2016}, there is one possible direction presented for research in this area -- the replacement of discrete data produced by finite element solver by continuous functions. These approximation functions can be then described by a small number of parameters. The main goal is to find the areas in domain where the output discrete function has predictable development and can be easily replaced by a simple continuous function, e.g. linear function. Although this approach has remarkable results for some classes of functions it is not applicable to a general problem. For some special cases, such as functions with discontinuities, the method has poor results because approximation error is too high and -- what is more important -- it cannot be determined in advance.

In this study it was decided to apply purely algebraic approach. Various methods were considered, e.g. discrete wavelet transform \cite{Lui2001} and discrete cosine transform \cite{Watson1994}, for their successful use in image compression. SVD method \cite{Baker2005, Kalman1996, Golub1996} was chosen as the foundation of the compression algorithm. Considering that the results from the FEM analyses can be viewed as a series of arbitrary rectangular matrices, the implementation of compression algorithm based on SVD is straightforward as it can be applied to any rectangular matrix.

Compression methods usually yield approximated data. In the following text, the term \textit{approximation error} denotes an error resulted from compression, i.e. difference between original results of FEM analysis and their compressed form. It should not be confused with the error of the Finite Element Method itself that yields approximate solution to mathematical problems used to model physical reality. To quantify the approximation error, several error metrics were investigated. In \cite{SairaBanu2015}, there are some of them used in similar area of research. The ability to control the quality of compression was a key requirement for the implementation of the compression algorithm.

The quality of any compression method depends on the nature of input data. Therefore, several different non-trivial finite element analyses should be used as benchmarks for an implementation of the compression algorithm. Also, the size of input data (the output from the FEM analysis) should be large enough to test the algorithmic complexity under heavy load. Details about the analyses that were used as benchmarks can be found in \cite{Kruis2012, Koudelka2009, Koudelka2006, Kruis2005}.

Performance is very important aspect for development of the compression algorithm. SVD being very computational intensive, randomized algorithms for SVD decomposition were investigated \cite{Candes2011, Woolfe2008, Martinsson2011, Szlam2014}. Especially, the implementation described in \cite{Halko2011} was used in the compression algorithm for the optimization. Memory consumption and the suitable format of the output data from the compression algorithm also influence overall usability of the resulting work. Efficient data storage is connected with data structure \cite{Ivanyi2012, Ivanyi2014}. This area will be subject of further research.

Although the idea to use SVD for data compression is not new, it is applied in an entirely new area of post-processing of results from the FEM analysis. In contrast with image compression, where SVD was used in a lossy compression algorithms, the area of post-processing puts great emphasis on the mathematical accuracy of the approximation instead of the human perceptions of visualizations. Also, the storage and dimensions of the input data, and the variance of the values in data, are very different. The main contribution of this paper is therefore to explore this area and to offer a description of an implementation of the algorithm that is successfully used in the post-processing of large data.

% summary of sections
The rest of the paper is organized as follows. Section \ref{sec:math} contains mathematical background of the SVD compression (i.e. SVD method itself, its use in low-rank matrix approximation, and description of the randomized SVD method). Implementation of the compression algorithm is presented in Section \ref{sec:implementation}. Section \ref{sec:results} summarizes the results of the benchmarks that were designed to measure quality of the output from the compression algorithm, and also the performance of its implementation. The paper is concluded in Section \ref{sec:conclusion}.
