\section{Implementation}
\label{sec:implementation}

SVD decomposition is applied on matrices. The first thing to do is therefore to assemble an input matrix. Results from the Finite element method are discrete values calculated in nodes or integration points. There are usually multiple fields in result data (e.g. displacements, stress, strain etc.) and each field can have multiple components (e.g. displacements can have $X$, $Y$ and $Z$ component). As non-linear analyses usually lead to multiple computation steps there are multiple sets of data per each field component. All data corresponding to a single field component form the input matrix $\mtrx{A}$ to the compression algorithm. Only single field component is considered to fill the input matrix $\mtrx{A}$. Results for different data components (even for the same field) can be independent from each other. There can also be order-of-magnitude difference between the components and the compression algorithm could potentially clear the important information in the weaker component. Another reason against merging of components is computational complexity. The smaller the matrix $\mtrx{A}$ is the faster the decomposition algorithm performs.

Each row of matrix $\mtrx{A}$ corresponds to single time step and each column corresponds to a single node or integration point. Once the matrix $\mtrx{A}$ is built for a field component the compression algorithm can be applied on it. It is purely algebraic procedure, no information about geometry of the mesh is needed. 

Let us assume that the matrix is not empty and is full rank. Then from the formula (\ref{eq:cr-def}) follows that if $r$ equals to the rank of matrix $\mtrx{A}$ the compression ratio is always higher than one. In other words the memory consumption of stored decomposition is bigger than the size of the original matrix. To make the compression algorithm applicable the parameter $r$ must satisfy the condition

\begin{equation}
r<\frac{m n}{m+n+1}.
\label{eq:r-ineq}
\end{equation}

\noindent
Considering the usual shape of matrix containing FEM results this inequality is easily satisfiable even for the $r$ being close to the rank of the original matrix as in the typical case the number of nodes or integration points is much higher than the number of analysis steps and therefore $m \ll n$.

\subsection{Algorithm description}
Once the SVD decomposition is calculated the compression algorithm removes a certain number of singular values and corresponding singular vectors. The remaining singular values and vectors represent the compressed data. There are two strategies that influence the way how to get the number of singular values to be preserved -- resulting size and quality. Each strategy is assigned a control parameter that determines compression ratio or approximation error.

\paragraph{Compression ratio}
If the focus is only on the size of compressed data, the rank $r$ of the approximation matrix can be calculated by the formula

\begin{equation}
r=\ceil*{c \times \frac{m n}{m+n+1}},
\label{eq:rank-from-comp-ratio}
\end{equation}

\noindent
where $c$ is the compression ratio, $0 \leq c \leq 1$ ($0$ results in absolute compression while $1$ results in no compression); $\ceil*{.}$ is the ceiling function.

\paragraph{Approximation error}
In the usual case the most important measure to take into account is the approximation error. Algorithm is trying to minimize the compression ratio while at the same time ensuring that predefined approximation error threshold is not exceeded. To quantify the error the Normalized root-mean-square deviation ($NRMSD$) is used. The normalized error metric enables to work with various data sets that have different scales. $NRMSD$ is defined in section \ref{sec:error}.

To effectively calculate the final rank of the approximation matrix from the desired approximation error the interesting property of singular values

% TODO: Reference to source or further explanation needed. (prof. Marek?)

\begin{equation}
\sum_{i=1}^{m} \sum_{j=1}^{n} (a_{ij})^{2} = \sum_{i=1}^{k}{s_{i}^{2}},
\label{eq:elem-sqr-sigma-sqr}
\end{equation}

\noindent
where $k=min(m, n)$, i.e. the smallest of two dimensions of the matrix $\mtrx{A}$, is made use of. The above formula states that the sum of squared elements of matrix $\mtrx{A}$ equals to the sum of squared singular values $s_{i}$ of the same matrix $\mtrx{A}$.

Using formulas \eqref{eq:svd-expansion} and \eqref{eq:svd-approx-expansion} the equation \eqref{eq:elem-sqr-sigma-sqr} can be applied to the difference between original matrix $\mtrx{A}$ and approximation matrix $\mtrx{A'}$.

\begin{equation}
\sum_{i=1}^{m} \sum_{j=1}^{n} (a_{ij} - a'_{ij})^{2} = \sum_{i=r+1}^{k}{s_{i}^{2}},
\end{equation}

\noindent
where the term on the right-hand side is the sum of squares of those singular values of the matrix $\mtrx{A}$ that are going to be cut away by the compression algorithm. The equation can be rewritten using the definition of $MSE$ in \eqref{eq:mse-def} to

\begin{equation}
%MSE \times m n = s_{r+1}^{2} + s_{r+2}^{2} + ... + s_{k-1}^{2} + s_{k}^2
MSE \times m n = \sum_{i=r+1}^{k} s_{i}^{2}
\end{equation}

\noindent
and using \eqref{eq:nrmsd-def} further to

\begin{equation}
%(NRMSD \times (X_{max}-X_{min}))^{2} \times m n = s_{r+1}^{2} + s_{r+2}^{2} + ... + s_{k-1}^{2} + s_{k}^2.
(NRMSD \times (X_{max}-X_{min}))^{2} \times m n = \sum_{i=r+1}^{k} s_{i}^{2}.
\end{equation}

Then $NRMSD$ can be used as a quality metric for the compression algorithm because normalization makes it usable for different datasets. Calculation of rank of the approximation matrix is depicted as pseudo-code in Algorithm \ref{alg:rank-calculation}. Algorithm uses the following inequality to test whether the desired rank has been reached

\begin{equation}
e > \frac{\sqrt[]{\frac{\sum_{i=r+1}^{k} s_{i}^{2}}{m n}}}{X_{max}-X_{min}},
\end{equation}

\noindent
where $e$ is NRMSD used as an error limit that can not be exceeded to achieve reasonable quality of approximation.

\begin{algorithm}
  \caption{Calculation of rank for approximation matrix from maximum allowed error}\label{rankAlgorithm}
  \label{alg:rank-calculation}
  \begin{algorithmic}[1]
  	\INPUT maximum allowed error ($e$), array with singular values ($S$), element count ($c$), maximum element value ($x_{max}$), minimum element value ($x_{min}$)
    \OUTPUT rank of resulting matrix
    \Procedure{CalculateRank}{$e, S, c, x_{max}, x_{min}$}
      \State $MSE \gets 0$
      \State $NRMSD \gets 0$
      \State $rank \gets S.length$
      \While{$NRMSD < e$}\Comment{repeat until max error is reached}
        \State $MSE \gets MSE + S[rank]/c$ \Comment{calculate MSE for current rank}
        \State $NRMSD \gets \sqrt{MSE} / (x_{max} - x_{min})$ \Comment{normalize error}
        \State $rank \gets rank - 1$ \Comment{decrement rank for next loop}
      \EndWhile
      \State \textbf{return} $rank + 1$ \Comment{Add one to not exceed maximum allowed error}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Optimization}

% http://mathoverflow.net/questions/161252/what-is-the-time-complexity-of-truncated-svd

Computational complexity of the exact SVD decomposition algorithm is $O(m^2n)$, where $m<n$. This theoretical algorithm complexity is confirmed by two benchmarks where the dependency of execution time on varying matrix dimension is shown. The results of the benchmarks are depicted in Figure \ref{fig:ExeTime_rows} and Figure \ref{fig:ExeTime_columns}. Several observations were made from the results:

\begin{itemize}
\item The algorithm is most efficient in cases where one dimension of the input matrix is very low compared to the other. However, this is almost always the case when compressing results from FEM -- number of time steps seldom exceeds hundreds.
\item Moreover, time steps can be split into smaller ranges and the algorithm can be applied on each range separately. This will improve performance and can also increase quality of compression if the key time steps on the range boundaries are carefully selected.
\item Randomized SVD algorithm has the same order of algorithmic complexity but yet can significantly reduce execution time. (The implementation of the particular randomized SVD algorithm that was used is described in \cite{Halko2011}.)
\end{itemize}

Storage size of SVD decomposition itself can also be optimized. $\mtrx{S}$ being a diagonal matrix can be stored as single list of singular values $s_{i}$ or can be even multiplied with the matrix of left singular vectors $\mtrx{U}$.

% main features for optimization: key time steps (time step span compression), Randomized SVD, Parallelization, Sparse matrix of details, prenasobeni U matice singularnimi cisly, trochu usetrim pamet, mohu pouzit vzorkovani...
